Multiprocessing 启动方法已设置为 'spawn'。
检测到 128 个 CPU 核心, 将使用 14 个并行工作进程。
[Main] 主进程使用 device: cuda
未找到模型路径或 model_path 为 None。将从头开始训练。
Meta-agent Q-Net 已创建并移至 CPU 共享内存。
正在创建 14 个工作进程的进程池...
进程池创建完毕。
开始 Reptile 元学习训练 (目标 500000 个任务, 异步批大小 14)...
Tasks 112/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0017 | Meta Epsilon: 0.9975 | Total Train Steps: 56320 | Tasks/sec: 0.10 | Timestamp: 20251031 22:02:26
Tasks 210/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.9952 | Total Train Steps: 105984 | Tasks/sec: 0.10 | Timestamp: 20251031 22:18:36
Tasks 308/500000 | Avg Reward (last 100): 0.67 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.9930 | Total Train Steps: 155648 | Tasks/sec: 0.10 | Timestamp: 20251031 22:33:59
Tasks 406/500000 | Avg Reward (last 100): 0.69 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.9908 | Total Train Steps: 205312 | Tasks/sec: 0.10 | Timestamp: 20251031 22:49:36
Tasks 504/500000 | Avg Reward (last 100): 0.69 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.9886 | Total Train Steps: 255488 | Tasks/sec: 0.10 | Timestamp: 20251031 23:05:30
Tasks 602/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.9864 | Total Train Steps: 304640 | Tasks/sec: 0.10 | Timestamp: 20251031 23:21:12
Tasks 700/500000 | Avg Reward (last 100): 0.69 | Avg Loss (last 100): 0.0038 | Meta Epsilon: 0.9842 | Total Train Steps: 354816 | Tasks/sec: 0.10 | Timestamp: 20251031 23:36:57
Tasks 812/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0034 | Meta Epsilon: 0.9817 | Total Train Steps: 411136 | Tasks/sec: 0.10 | Timestamp: 20251031 23:54:46
Tasks 910/500000 | Avg Reward (last 100): 0.70 | Avg Loss (last 100): 0.0032 | Meta Epsilon: 0.9795 | Total Train Steps: 460800 | Tasks/sec: 0.10 | Timestamp: 20251101 00:10:23
Tasks 1008/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0029 | Meta Epsilon: 0.9773 | Total Train Steps: 510464 | Tasks/sec: 0.10 | Timestamp: 20251101 00:26:41

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 1008) ---
Tasks 1106/500000 | Avg Reward (last 100): 0.68 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.9751 | Total Train Steps: 560128 | Tasks/sec: 0.10 | Timestamp: 20251101 00:41:52
Tasks 1204/500000 | Avg Reward (last 100): 0.67 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.9729 | Total Train Steps: 610304 | Tasks/sec: 0.10 | Timestamp: 20251101 00:57:44
Tasks 1302/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.9707 | Total Train Steps: 659968 | Tasks/sec: 0.10 | Timestamp: 20251101 01:13:30
Tasks 1400/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.9686 | Total Train Steps: 709120 | Tasks/sec: 0.10 | Timestamp: 20251101 01:28:58
Tasks 1512/500000 | Avg Reward (last 100): 0.67 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.9661 | Total Train Steps: 765952 | Tasks/sec: 0.10 | Timestamp: 20251101 01:47:20
Tasks 1610/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.9640 | Total Train Steps: 815616 | Tasks/sec: 0.10 | Timestamp: 20251101 02:02:53
Tasks 1708/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.9619 | Total Train Steps: 863744 | Tasks/sec: 0.10 | Timestamp: 20251101 02:17:56
Tasks 1806/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.9597 | Total Train Steps: 913920 | Tasks/sec: 0.10 | Timestamp: 20251101 02:33:48
Tasks 1904/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.9576 | Total Train Steps: 963072 | Tasks/sec: 0.10 | Timestamp: 20251101 02:49:39
Tasks 2002/500000 | Avg Reward (last 100): 0.69 | Avg Loss (last 100): 0.0031 | Meta Epsilon: 0.9554 | Total Train Steps: 1012736 | Tasks/sec: 0.10 | Timestamp: 20251101 03:05:28

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 2002) ---
Tasks 2100/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.9533 | Total Train Steps: 1062912 | Tasks/sec: 0.10 | Timestamp: 20251101 03:21:00
Tasks 2212/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.9509 | Total Train Steps: 1119232 | Tasks/sec: 0.10 | Timestamp: 20251101 03:38:48
Tasks 2310/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.9488 | Total Train Steps: 1168384 | Tasks/sec: 0.10 | Timestamp: 20251101 03:55:02
Tasks 2408/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.9467 | Total Train Steps: 1217024 | Tasks/sec: 0.10 | Timestamp: 20251101 04:10:01
Tasks 2506/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.9446 | Total Train Steps: 1265664 | Tasks/sec: 0.10 | Timestamp: 20251101 04:25:26
Tasks 2604/500000 | Avg Reward (last 100): 0.67 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.9425 | Total Train Steps: 1314816 | Tasks/sec: 0.10 | Timestamp: 20251101 04:40:55
Tasks 2702/500000 | Avg Reward (last 100): 0.68 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.9404 | Total Train Steps: 1364480 | Tasks/sec: 0.10 | Timestamp: 20251101 04:56:39
Tasks 2800/500000 | Avg Reward (last 100): 0.69 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.9384 | Total Train Steps: 1413120 | Tasks/sec: 0.10 | Timestamp: 20251101 05:12:01
Tasks 2912/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.9360 | Total Train Steps: 1470464 | Tasks/sec: 0.10 | Timestamp: 20251101 05:30:14
Tasks 3010/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.9339 | Total Train Steps: 1520640 | Tasks/sec: 0.10 | Timestamp: 20251101 05:46:19

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 3010) ---
Tasks 3108/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.9318 | Total Train Steps: 1570304 | Tasks/sec: 0.10 | Timestamp: 20251101 06:02:13
Tasks 3206/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.9297 | Total Train Steps: 1619968 | Tasks/sec: 0.10 | Timestamp: 20251101 06:18:07
Tasks 3304/500000 | Avg Reward (last 100): 0.61 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.9276 | Total Train Steps: 1670144 | Tasks/sec: 0.10 | Timestamp: 20251101 06:33:51
Tasks 3402/500000 | Avg Reward (last 100): 0.58 | Avg Loss (last 100): 0.0020 | Meta Epsilon: 0.9255 | Total Train Steps: 1720320 | Tasks/sec: 0.10 | Timestamp: 20251101 06:49:39
Tasks 3500/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0019 | Meta Epsilon: 0.9234 | Total Train Steps: 1769984 | Tasks/sec: 0.10 | Timestamp: 20251101 07:05:48
Tasks 3612/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0030 | Meta Epsilon: 0.9211 | Total Train Steps: 1826816 | Tasks/sec: 0.10 | Timestamp: 20251101 07:23:21
Tasks 3710/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0033 | Meta Epsilon: 0.9190 | Total Train Steps: 1876992 | Tasks/sec: 0.10 | Timestamp: 20251101 07:39:23
Tasks 3808/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0019 | Meta Epsilon: 0.9170 | Total Train Steps: 1926656 | Tasks/sec: 0.10 | Timestamp: 20251101 07:55:22
Tasks 3906/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.9149 | Total Train Steps: 1976320 | Tasks/sec: 0.10 | Timestamp: 20251101 08:10:49
Tasks 4004/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.9128 | Total Train Steps: 2026496 | Tasks/sec: 0.10 | Timestamp: 20251101 08:27:01

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 4004) ---
Tasks 4102/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0019 | Meta Epsilon: 0.9108 | Total Train Steps: 2076672 | Tasks/sec: 0.10 | Timestamp: 20251101 08:42:41
Tasks 4200/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0029 | Meta Epsilon: 0.9088 | Total Train Steps: 2125824 | Tasks/sec: 0.10 | Timestamp: 20251101 08:58:50
Tasks 4312/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.9064 | Total Train Steps: 2182656 | Tasks/sec: 0.10 | Timestamp: 20251101 09:16:18
Tasks 4410/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.9044 | Total Train Steps: 2231808 | Tasks/sec: 0.10 | Timestamp: 20251101 09:32:10
Tasks 4508/500000 | Avg Reward (last 100): 0.57 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.9024 | Total Train Steps: 2281472 | Tasks/sec: 0.10 | Timestamp: 20251101 09:48:05
Tasks 4606/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.9004 | Total Train Steps: 2331136 | Tasks/sec: 0.10 | Timestamp: 20251101 10:03:24
Tasks 4704/500000 | Avg Reward (last 100): 0.67 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.8984 | Total Train Steps: 2380800 | Tasks/sec: 0.10 | Timestamp: 20251101 10:19:30
Tasks 4802/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.8964 | Total Train Steps: 2430976 | Tasks/sec: 0.10 | Timestamp: 20251101 10:35:17
Tasks 4900/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.8944 | Total Train Steps: 2480640 | Tasks/sec: 0.10 | Timestamp: 20251101 10:50:53
Tasks 5012/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.8921 | Total Train Steps: 2537472 | Tasks/sec: 0.10 | Timestamp: 20251101 11:09:14

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 5012) ---
Tasks 5110/500000 | Avg Reward (last 100): 0.68 | Avg Loss (last 100): 0.0033 | Meta Epsilon: 0.8901 | Total Train Steps: 2587136 | Tasks/sec: 0.10 | Timestamp: 20251101 11:24:52
Tasks 5208/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0029 | Meta Epsilon: 0.8881 | Total Train Steps: 2636288 | Tasks/sec: 0.10 | Timestamp: 20251101 11:40:34
Tasks 5306/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0019 | Meta Epsilon: 0.8862 | Total Train Steps: 2685440 | Tasks/sec: 0.10 | Timestamp: 20251101 11:56:32
Tasks 5404/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.8842 | Total Train Steps: 2735104 | Tasks/sec: 0.10 | Timestamp: 20251101 12:11:48
Tasks 5502/500000 | Avg Reward (last 100): 0.68 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.8822 | Total Train Steps: 2784768 | Tasks/sec: 0.10 | Timestamp: 20251101 12:27:37
Tasks 5600/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.8802 | Total Train Steps: 2834944 | Tasks/sec: 0.10 | Timestamp: 20251101 12:43:35
Tasks 5712/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0029 | Meta Epsilon: 0.8780 | Total Train Steps: 2891264 | Tasks/sec: 0.10 | Timestamp: 20251101 13:01:34
Tasks 5810/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.8760 | Total Train Steps: 2940928 | Tasks/sec: 0.10 | Timestamp: 20251101 13:17:01
Tasks 5908/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.8741 | Total Train Steps: 2990592 | Tasks/sec: 0.10 | Timestamp: 20251101 13:32:53
Tasks 6006/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.8721 | Total Train Steps: 3040256 | Tasks/sec: 0.10 | Timestamp: 20251101 13:48:46

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 6006) ---
Tasks 6104/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0015 | Meta Epsilon: 0.8702 | Total Train Steps: 3089408 | Tasks/sec: 0.10 | Timestamp: 20251101 14:04:27
Tasks 6202/500000 | Avg Reward (last 100): 0.68 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.8683 | Total Train Steps: 3139072 | Tasks/sec: 0.10 | Timestamp: 20251101 14:19:58
Tasks 6300/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.8663 | Total Train Steps: 3188736 | Tasks/sec: 0.10 | Timestamp: 20251101 14:35:59
Tasks 6412/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.8641 | Total Train Steps: 3245568 | Tasks/sec: 0.10 | Timestamp: 20251101 14:53:51
Tasks 6510/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.8622 | Total Train Steps: 3295744 | Tasks/sec: 0.10 | Timestamp: 20251101 15:09:41
Tasks 6608/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0020 | Meta Epsilon: 0.8602 | Total Train Steps: 3345408 | Tasks/sec: 0.10 | Timestamp: 20251101 15:25:14
Tasks 6706/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.8583 | Total Train Steps: 3395072 | Tasks/sec: 0.10 | Timestamp: 20251101 15:41:28
Tasks 6804/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0031 | Meta Epsilon: 0.8564 | Total Train Steps: 3445248 | Tasks/sec: 0.10 | Timestamp: 20251101 15:57:10
Tasks 6902/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.8545 | Total Train Steps: 3495424 | Tasks/sec: 0.10 | Timestamp: 20251101 16:13:39
Tasks 7000/500000 | Avg Reward (last 100): 0.61 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.8525 | Total Train Steps: 3545600 | Tasks/sec: 0.10 | Timestamp: 20251101 16:29:02

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 7000) ---
Tasks 7112/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.8503 | Total Train Steps: 3602944 | Tasks/sec: 0.10 | Timestamp: 20251101 16:47:29
Tasks 7210/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0023 | Meta Epsilon: 0.8484 | Total Train Steps: 3652608 | Tasks/sec: 0.10 | Timestamp: 20251101 17:03:13
Tasks 7308/500000 | Avg Reward (last 100): 0.56 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.8465 | Total Train Steps: 3702784 | Tasks/sec: 0.10 | Timestamp: 20251101 17:19:21
Tasks 7406/500000 | Avg Reward (last 100): 0.54 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.8446 | Total Train Steps: 3752448 | Tasks/sec: 0.10 | Timestamp: 20251101 17:34:46
Tasks 7504/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.8427 | Total Train Steps: 3802112 | Tasks/sec: 0.10 | Timestamp: 20251101 17:51:03
Tasks 7602/500000 | Avg Reward (last 100): 0.61 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.8408 | Total Train Steps: 3852288 | Tasks/sec: 0.10 | Timestamp: 20251101 18:06:57
Tasks 7700/500000 | Avg Reward (last 100): 0.58 | Avg Loss (last 100): 0.0018 | Meta Epsilon: 0.8390 | Total Train Steps: 3901952 | Tasks/sec: 0.10 | Timestamp: 20251101 18:22:35
Tasks 7812/500000 | Avg Reward (last 100): 0.61 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.8368 | Total Train Steps: 3959296 | Tasks/sec: 0.10 | Timestamp: 20251101 18:40:59
Tasks 7910/500000 | Avg Reward (last 100): 0.65 | Avg Loss (last 100): 0.0033 | Meta Epsilon: 0.8350 | Total Train Steps: 4008448 | Tasks/sec: 0.10 | Timestamp: 20251101 18:56:24
Tasks 8008/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.8331 | Total Train Steps: 4057088 | Tasks/sec: 0.10 | Timestamp: 20251101 19:12:08

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 8008) ---
Tasks 8106/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.8313 | Total Train Steps: 4106240 | Tasks/sec: 0.10 | Timestamp: 20251101 19:27:25
Tasks 8204/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0024 | Meta Epsilon: 0.8295 | Total Train Steps: 4154368 | Tasks/sec: 0.10 | Timestamp: 20251101 19:42:32
Tasks 8302/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0026 | Meta Epsilon: 0.8276 | Total Train Steps: 4204544 | Tasks/sec: 0.10 | Timestamp: 20251101 19:58:25
Tasks 8400/500000 | Avg Reward (last 100): 0.61 | Avg Loss (last 100): 0.0022 | Meta Epsilon: 0.8258 | Total Train Steps: 4254208 | Tasks/sec: 0.10 | Timestamp: 20251101 20:14:21
Tasks 8512/500000 | Avg Reward (last 100): 0.58 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.8236 | Total Train Steps: 4311552 | Tasks/sec: 0.10 | Timestamp: 20251101 20:33:14
Tasks 8610/500000 | Avg Reward (last 100): 0.61 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.8218 | Total Train Steps: 4361728 | Tasks/sec: 0.10 | Timestamp: 20251101 20:48:39
Tasks 8708/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.8199 | Total Train Steps: 4411904 | Tasks/sec: 0.10 | Timestamp: 20251101 21:04:34
Tasks 8806/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.8181 | Total Train Steps: 4461568 | Tasks/sec: 0.10 | Timestamp: 20251101 21:20:34
Tasks 8904/500000 | Avg Reward (last 100): 0.55 | Avg Loss (last 100): 0.0020 | Meta Epsilon: 0.8163 | Total Train Steps: 4511232 | Tasks/sec: 0.10 | Timestamp: 20251101 21:36:20
Tasks 9002/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0027 | Meta Epsilon: 0.8145 | Total Train Steps: 4560896 | Tasks/sec: 0.10 | Timestamp: 20251101 21:51:57

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 9002) ---
Tasks 9100/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0037 | Meta Epsilon: 0.8126 | Total Train Steps: 4610560 | Tasks/sec: 0.10 | Timestamp: 20251101 22:07:53
Tasks 9212/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0019 | Meta Epsilon: 0.8106 | Total Train Steps: 4667392 | Tasks/sec: 0.10 | Timestamp: 20251101 22:25:51
Tasks 9310/500000 | Avg Reward (last 100): 0.58 | Avg Loss (last 100): 0.0025 | Meta Epsilon: 0.8087 | Total Train Steps: 4717568 | Tasks/sec: 0.10 | Timestamp: 20251101 22:41:56
Tasks 9408/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0034 | Meta Epsilon: 0.8069 | Total Train Steps: 4767232 | Tasks/sec: 0.10 | Timestamp: 20251101 22:57:52
Tasks 9506/500000 | Avg Reward (last 100): 0.59 | Avg Loss (last 100): 0.0017 | Meta Epsilon: 0.8051 | Total Train Steps: 4817408 | Tasks/sec: 0.10 | Timestamp: 20251101 23:13:56
Tasks 9604/500000 | Avg Reward (last 100): 0.60 | Avg Loss (last 100): 0.0021 | Meta Epsilon: 0.8033 | Total Train Steps: 4867072 | Tasks/sec: 0.10 | Timestamp: 20251101 23:29:04
Tasks 9702/500000 | Avg Reward (last 100): 0.66 | Avg Loss (last 100): 0.0033 | Meta Epsilon: 0.8015 | Total Train Steps: 4916736 | Tasks/sec: 0.10 | Timestamp: 20251101 23:45:36
Tasks 9800/500000 | Avg Reward (last 100): 0.64 | Avg Loss (last 100): 0.0028 | Meta Epsilon: 0.7997 | Total Train Steps: 4966400 | Tasks/sec: 0.10 | Timestamp: 20251102 00:00:54
Tasks 9912/500000 | Avg Reward (last 100): 0.62 | Avg Loss (last 100): 0.0031 | Meta Epsilon: 0.7977 | Total Train Steps: 5023232 | Tasks/sec: 0.10 | Timestamp: 20251102 00:18:58
Tasks 10010/500000 | Avg Reward (last 100): 0.63 | Avg Loss (last 100): 0.0031 | Meta Epsilon: 0.7959 | Total Train Steps: 5072896 | Tasks/sec: 0.10 | Timestamp: 20251102 00:35:04

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 10010) ---

训练被中断。正在保存当前检查点...
Process SpawnPoolWorker-10:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 331, in train
    batch = self.replay_buffer.sample(self.batch_size)
  File "/root/mpreptile_optimized.py", line 261, in sample
    states_np = np.array(batch_states, dtype=np.float32)
KeyboardInterrupt
Process SpawnPoolWorker-6:
Process SpawnPoolWorker-11:
Process SpawnPoolWorker-9:
Traceback (most recent call last):
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 358, in train
    self.scaler.scale(loss).backward()
  File "/root/mpreptile_optimized.py", line 331, in train
    batch = self.replay_buffer.sample(self.batch_size)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/root/mpreptile_optimized.py", line 277, in sample
    batch_dones_tensor = torch.tensor(dones_np.reshape(-1, self.seq_len), device=self.device)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 103, in reptile_worker
    states_tensor = torch.tensor(obs_np, dtype=torch.float32, device=worker_device)
KeyboardInterrupt
Process SpawnPoolWorker-2:
Process SpawnPoolWorker-13:
Process SpawnPoolWorker-4:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 103, in reptile_worker
    states_tensor = torch.tensor(obs_np, dtype=torch.float32, device=worker_device)
KeyboardInterrupt
Traceback (most recent call last):
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 331, in train
    batch = self.replay_buffer.sample(self.batch_size)
  File "/root/mpreptile_optimized.py", line 275, in sample
    batch_actions_tensor = torch.tensor(actions_np.reshape(-1, self.seq_len), device=self.device)
KeyboardInterrupt
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 105, in reptile_worker
    actions_np, new_hidden_state = task_agent.select_actions(states_tensor, current_hidden_state)
  File "/root/mpreptile_optimized.py", line 328, in select_actions
    return actions.cpu().numpy(), new_hidden_state
KeyboardInterrupt
Process SpawnPoolWorker-3:
Process SpawnPoolWorker-14:
Process SpawnPoolWorker-12:
Process SpawnPoolWorker-5:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 103, in reptile_worker
    states_tensor = torch.tensor(obs_np, dtype=torch.float32, device=worker_device)
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 105, in reptile_worker
    actions_np, new_hidden_state = task_agent.select_actions(states_tensor, current_hidden_state)
  File "/root/mpreptile_optimized.py", line 328, in select_actions
    return actions.cpu().numpy(), new_hidden_state
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 359, in train
    self.scaler.step(self.optimizer)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 416, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 103, in reptile_worker
    states_tensor = torch.tensor(obs_np, dtype=torch.float32, device=worker_device)
KeyboardInterrupt
Process SpawnPoolWorker-8:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 359, in train
    self.scaler.step(self.optimizer)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 416, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
Process SpawnPoolWorker-7:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 359, in train
    self.scaler.step(self.optimizer)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 416, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt
Process SpawnPoolWorker-1:
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/root/miniconda3/lib/python3.10/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/root/miniconda3/lib/python3.10/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
  File "/root/mpreptile_optimized.py", line 137, in reptile_worker
    loss = task_agent.train()
  File "/root/mpreptile_optimized.py", line 359, in train
    self.scaler.step(self.optimizer)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 416, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in _maybe_opt_step
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
  File "/root/miniconda3/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 314, in <genexpr>
    if not sum(v.item() for v in optimizer_state["found_inf_per_device"].values()):
KeyboardInterrupt

--- 检查点已保存到 reptile_drqn_meta_agent_interrupt.pth (Tasks: 10069) ---
检查点已保存。安全退出。
正在关闭工作进程池...
